# Default model (gpt-3.5-turbo, gpt-4, ggml-gpt4all-j...).
default-model: flash
# Text to append when using the -f flag.
# format-text:
#   markdown: 'Format the response as markdown without enclosing backticks.'
#   json: 'Format the response as json without enclosing backticks.'
# List of predefined system messages that can be used as roles.
roles:
  "default": []
  shell:
    - file:///home/bjaskulski/.config/mods/shell.md
  learn:
    - file:///home/bjaskulski/.config/mods/learn.md
  proof:
    - file:///home/bjaskulski/.config/mods/proof.md
  prompt:
    - file:///home/bjaskulski/.config/mods/prompt.md
  coder:
    - file:///home/bjaskulski/.config/mods/coder.md
  short:
    - you are experienced senior developer
    - you always answer as short as possible
  math:
    - you are math teacher
    - always explain the answer step by step to your university student
    - prefer simpler terms and metaphors
    - leave your answer without decimals for square roots etc, unless told otherwise
# Ask for the response to be formatted as markdown unless otherwise set.
format: false
# System role to use.
role: "default"
# Render output as raw text when connected to a TTY.
raw: false
# Quiet mode (hide the spinner while loading and stderr messages for success).
quiet: false
# Temperature (randomness) of results, from 0.0 to 2.0.
temp: 0.1
# TopP, an alternative to temperature that narrows response, from 0.0 to 1.0.
topp: 1.0
# Turn off the client-side limit on the size of the input into the model.
no-limit: false
# Wrap formatted output at specific width (default is 80)
word-wrap: 80
# Include the prompt from the arguments in the response.
include-prompt-args: false
# Include the prompt from the arguments and stdin, truncate stdin to specified number of lines.
include-prompt: 0
# Maximum number of times to retry API calls.
max-retries: 5
# Your desired level of fanciness.
fanciness: 0
# Text to show while generating.
status-text: Generating
# Default character limit on input to model.
max-input-chars: 12250
# Maximum number of tokens in response.
# max-tokens: 100
# Aliases and endpoints for OpenAI compatible REST API.
apis:
  openai:
    base-url: https://api.openai.com/v1
    api-key:
    api-key-env: OPENAI_API_KEY
    models:
      gpt-4:
        aliases: ["4"]
        max-input-chars: 24500
        fallback: gpt-3.5-turbo
  groq:
    base-url: https://api.groq.com/openai/v1
    api-key-env: GROQ_API_KEY
    models:
      llama-3.1-70b-versatile:
        aliases: ["llama3.1"]
        max-input-chars: 392000
      llama-3.2-90b-text-preview:
        aliases: ["llama3.2", "l3.2"]
        max-input-chars: 392000
      llama-3.3-70b-versatile:
        aliases: ["llama3.3", "l3"]
        max-input-chars: 392000
  anthropic:
    base-url: https://api.anthropic.com/v1
    api-key:
    api-key-env: ANTHROPIC_API_KEY
    models: # https://docs.anthropic.com/en/docs/about-claude/models
      claude-3-7-sonnet-latest:
        aliases: ["sonnet"]
        max-input-chars: 680000
      claude-3-5-haiku-20241022:
        aliases: ["haiku"]
        max-input-chars: 680000
  google:
    models:
      gemini-2.0-pro-exp-02-05:
        aliases: ["gemini"]
        max-input-chars: 392000
      gemini-2.0-flash:
        aliases: ["flash", "f"]
        max-input-chars: 392000
      gemini-2.0-flash-thinking-exp:
        aliases: ["think", "t"]
        max-input-chars: 392000
      learnlm-1.5-pro-experimental:
        aliases: ["learn", "l"]
        max-input-chars: 392000
  deepseek:
    base-url: https://api.deepseek.com/
    api-key:
    api-key-env: DEEPSEEK_API_KEY
    models: # https://platform.deepseek.com/api-docs/api/list-models/
      deepseek-chat:
        aliases: ["ds-chat"]
        max-input-chars: 384000
      deepseek-code:
        aliases: ["ds-code"]
        max-input-chars: 384000
  perplexica:
    base-url: http://localhost:3001/
    models:
      default:
        aliases: ["perplexica"]
        max-input-chars: 32000
